# Distributed Transactions: Architecture Guide

## Overview

| Attribute | Details |
|-----------|---------|
| Duration | 60 minutes |
| Level | Intermediate to Advanced |
| Prerequisites | Database fundamentals, microservices basics |

## Learning Objectives

- Understand why distributed transactions are fundamentally hard
- Compare 2PC, 3PC, Saga, and Eventual Consistency patterns
- Make informed architectural trade-offs for your use case
- Design compensation and recovery strategies
- Handle edge cases and failure scenarios

---

## 1. The Distributed Transaction Challenge

### Why It's Fundamentally Hard

~~~
Monolithic (Easy):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Single Database              â”‚
â”‚  BEGIN TRANSACTION                      â”‚
â”‚    UPDATE accounts SET balance -= 100   â”‚
â”‚    UPDATE accounts SET balance += 100   â”‚
â”‚  COMMIT                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘ ACID guaranteed by DB

Distributed (Hard):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Order   â”‚    â”‚ Payment  â”‚    â”‚Inventory â”‚
â”‚   DB1    â”‚â”€â”€â”€â–¶â”‚   DB2    â”‚â”€â”€â”€â–¶â”‚   DB3    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘ No single authority to guarantee ACID
~~~

**Core Questions:**
- What if Payment succeeds but Inventory fails?
- What if network partitions during commit?
- What if a service crashes mid-transaction?

### CAP Theorem: The Fundamental Trade-off

~~~mermaid
flowchart TB
    subgraph CAP["CAP Theorem: Pick Two"]
        C["Consistency<br/>All nodes see same data"]
        A["Availability<br/>System always responds"]
        P["Partition Tolerance<br/>Works despite network splits"]
    end
    
    C ---|"CA: Single DB<br/>(not distributed)"| A
    A ---|"AP: Saga, Eventual<br/>(may have stale reads)"| P
    P ---|"CP: 2PC, Locks<br/>(blocks during partition)"| C
~~~

| Choice | What You Get | What You Sacrifice | Example |
|--------|--------------|-------------------|---------|
| CP | Strong consistency | Availability during partitions | 2PC, distributed locks |
| AP | High availability | Immediate consistency | Saga, eventual consistency |
| CA | Both C and A | Not truly distributed | Single PostgreSQL |

> **Reality:** Network partitions WILL happen â†’ You must choose between C and A

---

## 2. Two-Phase Commit (2PC)

### How It Works

~~~mermaid
sequenceDiagram
    participant CO as Coordinator
    participant A as Participant A
    participant B as Participant B
    participant C as Participant C
    
    rect rgb(230, 240, 255)
    Note over CO,C: Phase 1: PREPARE (Voting)
    CO->>A: PREPARE
    CO->>B: PREPARE
    CO->>C: PREPARE
    A-->>CO: YES (locks held)
    B-->>CO: YES (locks held)
    C-->>CO: YES (locks held)
    end
    
    Note over CO: All YES â†’ Decision: COMMIT
    
    rect rgb(220, 255, 220)
    Note over CO,C: Phase 2: COMMIT
    CO->>A: COMMIT
    CO->>B: COMMIT
    CO->>C: COMMIT
    A-->>CO: ACK (locks released)
    B-->>CO: ACK (locks released)
    C-->>CO: ACK (locks released)
    end
~~~

### 2PC State Machine

~~~
Coordinator States:
    INIT â†’ WAITING â†’ COMMITTED/ABORTED

Participant States:
    INIT â†’ PREPARED â†’ COMMITTED/ABORTED
              â†‘
         Locks held here (blocking!)
~~~

### Critical Problems with 2PC

| Problem | Scenario | Impact |
|---------|----------|--------|
| **Blocking** | Participants hold locks during voting | Throughput drops, deadlock risk |
| **Coordinator SPOF** | Coordinator crashes after PREPARE | Participants stuck indefinitely |
| **Network Partition** | Can't reach all participants | Transaction hangs |
| **Latency** | 2 round trips minimum | Not suitable for high-frequency |

### Edge Cases & Failure Scenarios

~~~
Scenario 1: Coordinator fails after sending PREPARE
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Coordinator: PREPARE sent â†’ CRASH                  â”‚
â”‚  Participant A: PREPARED (holding locks)            â”‚
â”‚  Participant B: PREPARED (holding locks)            â”‚
â”‚                                                     â”‚
â”‚  Result: Both participants BLOCKED indefinitely     â”‚
â”‚  Solution: Timeout + new coordinator election       â”‚
â”‚            But may cause inconsistency!             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Scenario 2: Participant fails after voting YES
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Participant A: YES â†’ CRASH â†’ RECOVERS              â”‚
â”‚                                                     â”‚
â”‚  On recovery, must:                                 â”‚
â”‚  1. Check transaction log                           â”‚
â”‚  2. Ask coordinator for decision                    â”‚
â”‚  3. If coordinator also crashed â†’ UNCERTAIN STATE   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Scenario 3: Network partition during Phase 2
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Coordinator: Sends COMMIT                          â”‚
â”‚  Participant A: Receives COMMIT âœ“                   â”‚
â”‚  Participant B: Network timeout âœ—                   â”‚
â”‚                                                     â”‚
â”‚  Result: A committed, B uncertain                   â”‚
â”‚  Must retry COMMIT to B until success               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
~~~

### When to Use 2PC

| âœ… Good Fit | âŒ Poor Fit |
|------------|-------------|
| Financial systems requiring strong consistency | High-throughput systems |
| Small number of participants (2-3) | Many microservices |
| Low-latency network (same datacenter) | Cross-region deployments |
| Batch processing jobs | User-facing real-time APIs |

---

## 3. Three-Phase Commit (3PC)

### Improvement Over 2PC

~~~mermaid
sequenceDiagram
    participant CO as Coordinator
    participant P as Participants
    
    rect rgb(230, 240, 255)
    Note over CO,P: Phase 1: CAN-COMMIT
    CO->>P: Can you commit?
    P-->>CO: YES/NO
    end
    
    rect rgb(255, 245, 220)
    Note over CO,P: Phase 2: PRE-COMMIT
    CO->>P: Prepare to commit
    P-->>CO: ACK
    Note over P: Key: Can commit on timeout!
    end
    
    rect rgb(220, 255, 220)
    Note over CO,P: Phase 3: DO-COMMIT
    CO->>P: Commit now
    P-->>CO: Done
    end
~~~

### 3PC vs 2PC Trade-offs

| Aspect | 2PC | 3PC |
|--------|-----|-----|
| Phases | 2 | 3 |
| Blocking on coordinator failure | Yes (indefinite) | No (timeout-based recovery) |
| Latency | Lower (2 RTT) | Higher (3 RTT) |
| Complexity | Medium | High |
| Network partition safety | Problematic | Still problematic |
| Practical adoption | Common (XA) | Rare |

### Why 3PC Isn't Widely Used

~~~
Problem: 3PC still fails under network partitions

Scenario: Network splits during PRE-COMMIT
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Partition A: Coordinator + Participant 1           â”‚
â”‚  Partition B: Participant 2, 3                      â”‚
â”‚                                                     â”‚
â”‚  Partition A: Times out â†’ COMMIT (has majority?)    â”‚
â”‚  Partition B: Times out â†’ COMMIT (assumed safe)     â”‚
â”‚                                                     â”‚
â”‚  Result: Both partitions may make different         â”‚
â”‚          decisions â†’ INCONSISTENCY                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Reality: Network partitions are common in distributed systems
         â†’ 3PC doesn't solve the fundamental problem
         â†’ Industry moved to Saga/Eventual Consistency instead
~~~

---

## 4. Saga Pattern

### Core Concept: Local Transactions + Compensation

~~~mermaid
flowchart LR
    subgraph forward["Forward Flow (Happy Path)"]
        T1["T1: Create Order"] --> T2["T2: Process Payment"]
        T2 --> T3["T3: Reserve Inventory"]
        T3 --> T4["T4: Arrange Shipping"]
    end
~~~

~~~mermaid
flowchart RL
    subgraph compensation["Compensation Flow (T3 Fails)"]
        F["T3 Failed âŒ"] --> C2["C2: Refund Payment"]
        C2 --> C1["C1: Cancel Order"]
    end
~~~

### Key Principle

~~~
Each step Ti has a compensating transaction Ci

If Tn fails:
  Execute Cn-1, Cn-2, ... C1 in reverse order
  
Important: Compensation â‰  Rollback
  - Rollback: Undo as if never happened
  - Compensation: Apply corrective action (visible in history)
~~~

### Choreography vs Orchestration

~~~mermaid
flowchart TB
    subgraph choreo["Choreography: Event-Driven"]
        O1["Order Service"] -->|"OrderCreated"| E1["Event Bus"]
        E1 -->|"OrderCreated"| P1["Payment Service"]
        P1 -->|"PaymentDone"| E1
        E1 -->|"PaymentDone"| I1["Inventory Service"]
    end
~~~

~~~mermaid
flowchart TB
    subgraph orch["Orchestration: Central Control"]
        ORCH["Saga Orchestrator"]
        ORCH -->|"1. CreateOrder"| O2["Order Service"]
        ORCH -->|"2. ProcessPayment"| P2["Payment Service"]
        ORCH -->|"3. ReserveInventory"| I2["Inventory Service"]
    end
~~~

### Choreography vs Orchestration Trade-offs

| Aspect | Choreography | Orchestration |
|--------|--------------|---------------|
| Coupling | Loose | Tighter |
| Single Point of Failure | No | Yes (orchestrator) |
| Visibility | Hard to track flow | Easy to monitor |
| Debugging | Difficult | Straightforward |
| Adding new steps | Modify multiple services | Modify orchestrator only |
| Cyclic dependencies | Risk of event loops | Not possible |
| Team autonomy | High | Lower |

### Architecture Decision Guide

~~~
Choose CHOREOGRAPHY when:
â”œâ”€â”€ Teams are autonomous and own their services
â”œâ”€â”€ Flow is simple (< 4 steps)
â”œâ”€â”€ Services are truly independent
â””â”€â”€ You have good distributed tracing

Choose ORCHESTRATION when:
â”œâ”€â”€ Flow is complex (> 4 steps)
â”œâ”€â”€ Business logic is centralized
â”œâ”€â”€ You need clear visibility/monitoring
â”œâ”€â”€ Compensation logic is complex
â””â”€â”€ Regulatory/audit requirements exist
~~~

### Critical Edge Cases

#### Edge Case 1: Compensation Fails

~~~
Scenario: Payment refund fails during compensation
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  T1: Order Created âœ“                                â”‚
â”‚  T2: Payment Processed âœ“                            â”‚
â”‚  T3: Inventory Reserve FAILED âœ—                     â”‚
â”‚  C2: Refund Payment FAILED âœ—  â† What now?           â”‚
â”‚                                                     â”‚
â”‚  Solutions:                                         â”‚
â”‚  1. Retry with exponential backoff                  â”‚
â”‚  2. Dead letter queue for manual intervention       â”‚
â”‚  3. Scheduled reconciliation job                    â”‚
â”‚  4. Alert operations team                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
~~~

#### Edge Case 2: Duplicate Execution

~~~
Scenario: Network timeout, message redelivered
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Payment Service receives "ProcessPayment" twice    â”‚
â”‚                                                     â”‚
â”‚  Without idempotency:                               â”‚
â”‚  â†’ Customer charged twice! ğŸ’€                       â”‚
â”‚                                                     â”‚
â”‚  Solution: Idempotency keys                         â”‚
â”‚  1. Each request has unique idempotency_key         â”‚
â”‚  2. Store processed keys in database                â”‚
â”‚  3. Check before processing, skip if exists         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
~~~

#### Edge Case 3: Out-of-Order Events

~~~
Scenario: Events arrive out of order
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Expected: OrderCreated â†’ PaymentDone â†’ Shipped     â”‚
â”‚  Actual:   PaymentDone â†’ OrderCreated â†’ Shipped     â”‚
â”‚                                                     â”‚
â”‚  Solutions:                                         â”‚
â”‚  1. Event versioning/sequencing                     â”‚
â”‚  2. State machine validation                        â”‚
â”‚  3. Buffer and reorder                              â”‚
â”‚  4. Reject and retry later                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
~~~

#### Edge Case 4: Long-Running Transactions

~~~
Scenario: Shipping takes 3 days
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Problem: Can't hold resources for days             â”‚
â”‚                                                     â”‚
â”‚  Solutions:                                         â”‚
â”‚  1. Reservation pattern (soft lock with expiry)    â”‚
â”‚  2. Split into sub-sagas                            â”‚
â”‚  3. State machine with timeout transitions          â”‚
â”‚  4. Async notification when complete                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
~~~

### Saga State Machine Design

~~~mermaid
stateDiagram-v2
    [*] --> STARTED
    STARTED --> ORDER_CREATED: createOrder()
    ORDER_CREATED --> PAYMENT_PROCESSED: processPayment()
    PAYMENT_PROCESSED --> INVENTORY_RESERVED: reserveInventory()
    INVENTORY_RESERVED --> COMPLETED: success
    
    ORDER_CREATED --> COMPENSATING: failure
    PAYMENT_PROCESSED --> COMPENSATING: failure
    INVENTORY_RESERVED --> COMPENSATING: failure
    
    COMPENSATING --> COMPENSATED: all compensations done
    COMPENSATING --> COMPENSATION_FAILED: compensation fails
    
    COMPLETED --> [*]
    COMPENSATED --> [*]
    COMPENSATION_FAILED --> MANUAL_INTERVENTION
~~~

---

## 5. Eventual Consistency & Outbox Pattern

### The Dual Write Problem

```mermaid
flowchart LR
    S["Service"] -->|"1. Write DB âœ“"| DB[(Database)]
    S -->|"2. Publish Event âœ—"| MQ[Message Broker]
    
    style MQ stroke:#ff0000,stroke-width:2px
~~~

~~~
Problem: Two separate systems, no atomic guarantee

Failure scenarios:
1. DB write succeeds, event publish fails
   â†’ Data saved but other services never notified

2. Event published, DB write fails
   â†’ Other services act on non-existent data

3. Service crashes between the two operations
   â†’ Inconsistent state
~~~

### Transactional Outbox Pattern

```mermaid
flowchart TB
    S["Service"] --> TX
    
    subgraph TX["Single Database Transaction"]
        W1["1. Write business data"]
        W2["2. Write event to outbox table"]
    end
    
    TX --> DB[(Database)]
    
    RELAY["Message Relay<br/>(Polling or CDC)"] -->|"Read outbox"| DB
    RELAY -->|"Publish"| KAFKA["Message Broker"]
~~~

### Outbox Table Design

~~~sql
CREATE TABLE outbox_events (
    id              UUID PRIMARY KEY,
    aggregate_type  VARCHAR(255),    -- e.g., 'Order'
    aggregate_id    VARCHAR(255),    -- e.g., order_id
    event_type      VARCHAR(255),    -- e.g., 'OrderCreated'
    payload         JSONB,           -- event data
    created_at      TIMESTAMP,
    published_at    TIMESTAMP NULL,  -- NULL = not yet published
    
    INDEX idx_unpublished (published_at) WHERE published_at IS NULL
);
~~~

### Message Relay Strategies

| Strategy | Pros | Cons |
|----------|------|------|
| **Polling** | Simple, no extra infrastructure | Latency, DB load |
| **CDC (Debezium)** | Real-time, low DB load | Complex setup |
| **Transaction log tailing** | Very efficient | DB-specific |

### CDC vs Polling Trade-offs

~~~
Polling:
â”œâ”€â”€ Latency: 1-5 seconds (configurable)
â”œâ”€â”€ DB Load: Constant queries
â”œâ”€â”€ Complexity: Low
â”œâ”€â”€ Ordering: Must handle carefully
â””â”€â”€ Best for: Simple setups, low volume

CDC (Change Data Capture):
â”œâ”€â”€ Latency: Milliseconds
â”œâ”€â”€ DB Load: Minimal (reads transaction log)
â”œâ”€â”€ Complexity: High (Kafka Connect, Debezium)
â”œâ”€â”€ Ordering: Guaranteed by log position
â””â”€â”€ Best for: High volume, low latency requirements
~~~

### Idempotent Consumer Pattern

~~~
Problem: Message broker may deliver same message twice
         (at-least-once delivery)

Solution: Track processed message IDs

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Consumer receives message                          â”‚
â”‚      â†“                                              â”‚
â”‚  Check: Is message_id in processed_messages table? â”‚
â”‚      â†“                                              â”‚
â”‚  YES â†’ Skip (already processed)                     â”‚
â”‚  NO  â†’ Process + Insert message_id + Commit         â”‚
â”‚                                                     â”‚
â”‚  All in single transaction!                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
~~~

---

## 6. Pattern Comparison & Selection Guide

### Comprehensive Comparison

| Aspect | 2PC | 3PC | Saga | Eventual Consistency |
|--------|-----|-----|------|---------------------|
| Consistency | Strong | Strong | Eventual | Eventual |
| Isolation | Full | Full | None | None |
| Availability | Low | Medium | High | High |
| Latency | High | Higher | Medium | Low |
| Scalability | Poor | Poor | Good | Excellent |
| Complexity | Medium | High | High | Medium |
| Recovery | Automatic | Timeout-based | Compensation | Retry + Idempotency |

### Decision Matrix

```mermaid
flowchart TB
    START["Need distributed transaction?"] --> Q1{"Strong consistency<br/>required?"}
    
    Q1 -->|"Yes"| Q2{"Can tolerate<br/>blocking?"}
    Q1 -->|"No"| Q3{"Complex multi-step<br/>workflow?"}
    
    Q2 -->|"Yes"| TWO_PC["2PC<br/>(XA Transactions)"]
    Q2 -->|"No"| CONSIDER["Consider relaxing<br/>consistency requirements"]
    
    Q3 -->|"Yes"| SAGA["Saga Pattern"]
    Q3 -->|"No"| OUTBOX["Outbox + Eventual<br/>Consistency"]
    
    SAGA --> Q4{"Need visibility<br/>& control?"}
    Q4 -->|"Yes"| ORCH["Orchestration"]
    Q4 -->|"No"| CHOREO["Choreography"]
~~~

### Industry Use Cases

| Company/Domain | Pattern | Reason |
|----------------|---------|--------|
| Banking (transfers) | 2PC or Saga with strict compensation | Regulatory, money involved |
| E-commerce (orders) | Saga (orchestration) | Complex flow, need visibility |
| Social media (posts) | Eventual consistency | High scale, consistency less critical |
| Ride-sharing (booking) | Saga (choreography) | Real-time, multiple services |
| Inventory systems | Saga + reservation pattern | Prevent overselling |

---

## 7. Production Considerations

### Monitoring & Observability

~~~
Essential Metrics:
â”œâ”€â”€ Saga completion rate
â”œâ”€â”€ Compensation frequency
â”œâ”€â”€ Average saga duration
â”œâ”€â”€ Failed/stuck sagas count
â”œâ”€â”€ Outbox lag (unpublished events)
â””â”€â”€ Message processing latency

Essential Logs:
â”œâ”€â”€ Saga state transitions
â”œâ”€â”€ Compensation triggers
â”œâ”€â”€ Retry attempts
â””â”€â”€ Timeout events

Distributed Tracing:
â”œâ”€â”€ Correlation ID across all services
â”œâ”€â”€ Span for each saga step
â””â”€â”€ Parent-child relationship for compensation
~~~

### Failure Recovery Strategies

~~~
Strategy 1: Automatic Retry
â”œâ”€â”€ Exponential backoff: 1s, 2s, 4s, 8s...
â”œâ”€â”€ Max retries: 3-5 typically
â”œâ”€â”€ Circuit breaker after threshold
â””â”€â”€ Alert on repeated failures

Strategy 2: Dead Letter Queue
â”œâ”€â”€ Move failed messages to DLQ
â”œâ”€â”€ Manual inspection and replay
â”œâ”€â”€ Audit trail preserved
â””â”€â”€ No blocking of other messages

Strategy 3: Scheduled Reconciliation
â”œâ”€â”€ Periodic job compares expected vs actual state
â”œâ”€â”€ Fixes inconsistencies automatically
â”œâ”€â”€ Reports discrepancies
â””â”€â”€ Last resort safety net
~~~

### Testing Distributed Transactions

~~~
Test Categories:
â”œâ”€â”€ Happy path (all services succeed)
â”œâ”€â”€ Single service failure
â”œâ”€â”€ Multiple service failures
â”œâ”€â”€ Network partition simulation
â”œâ”€â”€ Timeout scenarios
â”œâ”€â”€ Duplicate message handling
â”œâ”€â”€ Out-of-order message handling
â””â”€â”€ Compensation failure scenarios

Tools:
â”œâ”€â”€ Chaos engineering (Chaos Monkey, Litmus)
â”œâ”€â”€ Network fault injection (Toxiproxy)
â”œâ”€â”€ Contract testing (Pact)
â””â”€â”€ Integration test containers
~~~

---

## 8. Key Takeaways

| # | Takeaway |
|---|----------|
| 1 | **2PC provides strong consistency** but blocks and doesn't scale |
| 2 | **3PC reduces blocking** but still fails under network partitions |
| 3 | **Sagas trade isolation for availability** â€” design for compensation |
| 4 | **Choreography is loosely coupled** but hard to debug |
| 5 | **Orchestration centralizes logic** but creates SPOF |
| 6 | **Outbox pattern solves dual-write** â€” use CDC for production |
| 7 | **Design for idempotency** â€” messages will be delivered multiple times |
| 8 | **Compensation â‰  Rollback** â€” it's a corrective action, not undo |
| 9 | **Monitor saga health** â€” stuck sagas indicate systemic issues |
| 10 | **Choose based on requirements** â€” not all systems need strong consistency |

---

## 9. Practical Exercise

### Design Challenge

Design a distributed transaction strategy for a ride-sharing booking:

**Flow:**
1. User requests ride
2. Find available driver
3. Reserve driver (can't accept other rides)
4. Process payment authorization
5. Confirm booking

**Requirements:**
- Driver reservation expires after 30 seconds
- Payment failures should release driver
- Handle driver cancellation after booking
- Support concurrent booking attempts for same driver

**Discussion Questions:**
1. Choreography or orchestration? Why?
2. How do you handle "driver reserved but payment timeout"?
3. What if compensation (release driver) fails?
4. How do you prevent double-booking a driver?
5. How do you show booking status to user in real-time?